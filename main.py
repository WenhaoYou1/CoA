"""Main module for running the chain-of-agents summarization pipeline."""

import argparse
import json
from transformers import AutoTokenizer, AutoModelForCausalLM

import utils
from models.manager_agent import ManagerAgent
from models.worker_agent import WorkerAgent


def chain_of_agents_pipeline(
    source_text: str,
    query: str,
    k: int,
    query_based: bool,
    llm
) -> str:
    """Runs a multi-agent summarization pipeline.

    Args:
        source_text (str): The original text to be processed.
        query (str): The query for summarization.
        k (int): Number of chunks for processing.
        query_based (bool): Whether the summarization is query-based.
        llm: Tuple containing (model, tokenizer) for text generation.

    Returns:
        str: The final answer generated by the manager agent.
    """
    model, tokenizer = llm

    # Instantiate worker and manager agents
    worker = WorkerAgent(model=model, tokenizer=tokenizer)
    manager = ManagerAgent(model=model, tokenizer=tokenizer)
    # Set the instructions for worker agents
    instruction = (
    "You are a Worker agent. You read a chunk of the text and produce a summary. "
    "Then pass it to the next agent."
    )
    # Chunk the input text using correct parameter names.
    chunks = utils.chunk_text_by_sentence(
        text=source_text,
        query=query,
        k=k,
        instruction=instruction,
        tokenizer=tokenizer
    )

    # Stage 1: Worker Agents
    cu_prev = ""  # Initialize previous communication unit
    for chunk_text in chunks:
        cu_curr = worker.generate_summary(
            prev_summary=cu_prev,
            current_chunk=chunk_text,
            query=query
        )
        cu_prev = cu_curr

    # Stage 2: Manager Agent processes the final summary
    final_answer = manager.generate_response(
        final_communication_unit=cu_prev,
        query=query,
        query_based=query_based
    )

    return final_answer


def main(parsed_args):
    """Main function to run the chain-of-agents pipeline."""
    tokenizer = AutoTokenizer.from_pretrained(
        parsed_args.llm_name, trust_remote_code=True
    )
    model = AutoModelForCausalLM.from_pretrained(
        parsed_args.llm_name, device_map="auto", trust_remote_code=True
    )

    # Load data from JSON file.
    with open(parsed_args.json_file, "r", encoding="utf-8") as f:
        data = json.load(f)
    # Use the second sample as an example.
    sample = data[1]
    question = sample["question"]
    context_list = sample["context"]

    source_text = ""
    for item in context_list:
        title, sentences_list = item
        source_text += title + ": "
        for sentence in sentences_list:
            source_text += sentence

    final_answer = chain_of_agents_pipeline(
        source_text=source_text,
        query=question,
        k=parsed_args.window_size,
        query_based=parsed_args.query_based,
        llm=(model, tokenizer)
    )
    print("-------------------------")
    print("Question:", question)
    print("Answer:", final_answer)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chain-of-Agents")
    parser.add_argument(
        "--json_file", type=str, default="./datasets/hotpot_test_fullwiki_v1.json"
    )
    parser.add_argument("--llm_name", type=str, default="Qwen/Qwen-VL-Chat")
    parser.add_argument("--window_size", type=int, default=1024)
    parser.add_argument("--query_based", type=bool, default=True)

    args = parser.parse_args()
    main(args)
